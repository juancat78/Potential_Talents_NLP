{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeND426gnN7C"
   },
   "source": [
    "# **Potential Talents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbSmzmrQnVzX"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "As a talent sourcing and management company, we are interested in finding talented individuals for sourcing these candidates to technology companies. Finding talented candidates is not easy, for several reasons. The first reason is one needs to understand what the role is very well to fill in that spot, this requires understanding the client’s needs and what they are looking for in a potential candidate. The second reason is one needs to understand what makes a candidate shine for the role we are in search for. Third, where to find talented individuals is another challenge.\n",
    "\n",
    "The nature of our job requires a lot of human labor and is full of manual operations. Towards automating this process we want to build a better approach that could save us time and finally help us spot potential candidates that could fit the roles we are in search for. Moreover, going beyond that for a specific role we want to fill in we are interested in developing a machine learning powered pipeline that could spot talented individuals, and rank them based on their fitness.\n",
    "\n",
    "We are right now semi-automatically sourcing a few candidates, therefore the sourcing part is not a concern at this time but we expect to first determine best matching candidates based on how fit these candidates are for a given role. We generally make these searches based on some keywords such as “full-stack software engineer”, “engineering manager” or “aspiring human resources” based on the role we are trying to fill in. These keywords might change, and you can expect that specific keywords will be provided to you.\n",
    "\n",
    "Assuming that we were able to list and rank fitting candidates, we then employ a review procedure, as each candidate needs to be reviewed and then determined how good a fit they are through manual inspection. This procedure is done manually and at the end of this manual review, we might choose not the first fitting candidate in the list but maybe the 7th candidate in the list. If that happens, we are interested in being able to re-rank the previous list based on this information. This supervisory signal is going to be supplied by starring the 7th candidate in the list. Starring one candidate actually sets this candidate as an ideal candidate for the given role. Then, we expect the list to be re-ranked each time a candidate is starred.\n",
    "\n",
    "# Data Description:\n",
    "\n",
    "The data comes from our sourcing efforts. We removed any field that could directly reveal personal details and gave a unique identifier for each candidate.\n",
    "\n",
    "Attributes:\n",
    "id : unique identifier for candidate (numeric)\n",
    "\n",
    "job_title : job title for candidate (text)\n",
    "\n",
    "location : geographical location for candidate (text)\n",
    "\n",
    "connections: number of connections candidate has, 500+ means over 500 (text)\n",
    "\n",
    "Output (desired target):\n",
    "fit - how fit the candidate is for the role? (numeric, probability between 0-1)\n",
    "\n",
    "Keywords: “Aspiring human resources” or “seeking human resources”\n",
    "\n",
    "# Goal(s):\n",
    "\n",
    "Predict how fit the candidate is based on their available information (variable fit)\n",
    "\n",
    "# Success Metric(s):\n",
    "\n",
    "Rank candidates based on a fitness score.\n",
    "\n",
    "Re-rank candidates when a candidate is starred.\n",
    "\n",
    "# Current Challenges:\n",
    "\n",
    "We are interested in a robust algorithm, tell us how your solution works and show us how your ranking gets better with each starring action.\n",
    "\n",
    "How can we filter out candidates which in the first place should not be in this list?\n",
    "\n",
    "Can we determine a cut-off point that would work for other roles without losing high potential candidates?\n",
    "\n",
    "Do you have any ideas that we should explore so that we can even automate this procedure to prevent human bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkgjqhTjqFNa",
    "outputId": "0d23060b-0d5f-4366-e2c7-60044ffcc682"
   },
   "outputs": [],
   "source": [
    "# Montar Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqITHmoiqFjX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Ruta correcta al archivo CSV en Google Drive\n",
    "path_dbset = '/content/gdrive/MyDrive/Proyectos APZIVA/Potential Talents_Proy 3/potential_talents.csv'\n",
    "\n",
    "# Leer el archivo CSV usando pd.read_csv()\n",
    "db = pd.read_csv(path_dbset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfP-8whDrZX5"
   },
   "source": [
    "# **VISUALIZATION AND MISSING VALUE TREATMENT**\n",
    "# VISUALIZACIÓN Y TRATAMIENTO DE VALORES FALTANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HlkBOR3az107",
    "outputId": "9b56987b-45de-476d-a315-85d6d206e85c"
   },
   "outputs": [],
   "source": [
    "# Mostrar las primeras 10 filas\n",
    "print(db.head(7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JsX3vahqF1w",
    "outputId": "5093c47b-1820-4aa2-fbd2-3e7c69f88a72"
   },
   "outputs": [],
   "source": [
    "# verify information of the dataset\n",
    "db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9PAA9JJrzcg",
    "outputId": "00393400-1ed6-4ec8-9d55-27a65810d507"
   },
   "outputs": [],
   "source": [
    "# Rows and columns information. resultSet(rows, columns)\n",
    "db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jULlClEBsDh4",
    "outputId": "f33e0ad9-5d1b-4cb9-84ce-370e488d64b1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace missing value representations with NaN\n",
    "db.replace([\"?\", \"N/A\", \"NA\", \"null\", \"\"], np.nan, inplace=True)\n",
    "\n",
    "# Check for missing values (NaN) in absolute count\n",
    "print(\"\\nMissing values (NaN) per column (absolute count):\")\n",
    "print(db.isnull().sum())\n",
    "\n",
    "# Check for missing values (NaN) as a percentage\n",
    "print(\"\\nMissing values (NaN) per column (percentage):\")\n",
    "print((db.isnull().sum() / len(db)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obiGTRhkITL-"
   },
   "source": [
    "# **WORD2VEC IMPLEMENTATION**\n",
    "# IMPLEMENTACION DE WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyT0Nc0Lhep4"
   },
   "source": [
    "## **Preprocessing and tokenization**\n",
    "## Preprocesamiento y tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iI_cL7OKO6s",
    "outputId": "e4601e88-6751-4406-d8a2-bcd830065478"
   },
   "outputs": [],
   "source": [
    "#!pip uninstall -y numpy gensim\n",
    "#!pip install --upgrade numpy gensim\n",
    "#!pip uninstall -y gensim\n",
    "!pip install gensim==4.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOdCjd53f3Fx",
    "outputId": "4e25569e-c0fe-4b7b-fb45-bb541edcaa92"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "\n",
    "# Cargar el modelo de spaCy\n",
    "nlp = spacy.load('en_core_web_sm') # spacy es una librería donde esta el modelo preentrenado 'en_core_web_sm'.\n",
    "\n",
    "# la base de datos db tiene la columna con los job titles\n",
    "job_titles = db['job_title'].tolist()\n",
    "\n",
    "# Función para preprocesar los títulos de trabajo\n",
    "def preprocess_text(title):\n",
    "    doc = nlp(title.lower())  # Convertir a minúsculas y procesar con spaCy\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]  # Filtrar palabras clave\n",
    "    return tokens\n",
    "\n",
    "# Aplicar preprocesamiento\n",
    "tokenized_titles = [preprocess_text(title) for title in job_titles]\n",
    "\n",
    "# Ver los títulos tokenizados\n",
    "print(tokenized_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0udcaFOdKctY"
   },
   "source": [
    "SpaCy es una librería de procesamiento de lenguaje natural (NLP) en Python, y en_core_web_sm es uno de sus modelos preentrenados. Es como que spaCy es el \"motor\", y el modelo en_core_web_sm es uno de los \"combustibles\" que puede usar para procesar texto en inglés. Además es una librería en la que podemos hacer la limpieza de los títulos (llevar mayúsculas de las palabras que lo componen a minúsculas).\n",
    "spaCy = librería base (como scikit-learn, pero para texto).\n",
    "Modelos como en_core_web_sm = entrenados con muchos textos, para que spaCy pueda:\n",
    "-Tokenizar (separar palabras)\n",
    "-Lematizar (reducir palabras a su forma base)\n",
    "-Detectar entidades nombradas (NER)\n",
    "-Analizar sintaxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agnloyH4MS1P"
   },
   "source": [
    "La función def preprocess_text(title), limpia cada título de trabajo:\n",
    "\n",
    "title.lower() convierte el texto a minúsculas.\n",
    "\n",
    "nlp(title.lower()) lo analiza con spaCy (tokeniza, etiqueta, etc.).\n",
    "\n",
    "El bucle [token.lemma_ for token in doc ...] hace tres cosas:\n",
    "\n",
    "- Se queda con el lema de cada palabra (forma base, por ejemplo, \"driving\" -- \"drive\").\n",
    "\n",
    "- Filtra solo las palabras que son alfabéticas (token.is_alpha)-- se descartan números o símbolos.\n",
    "\n",
    "- Excluye las palabras vacías (como \"the\", \"and\", \"of\", etc.) usando not token.is_stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3jnHJ0GNAeN"
   },
   "source": [
    "Línea --> tokenized_titles = [preprocess_text(title) for title in job_titles]. Aplica la función anterior a cada título de la lista y guarda los resultados en tokenized_titles.\n",
    "Resultado: una lista de listas, donde cada sublista contiene los términos relevantes lematizados del título."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NZ5XBNmje88"
   },
   "outputs": [],
   "source": [
    "# Entrenar modelo Word2Vec\n",
    "word2vec_model = Word2Vec(sentences=tokenized_titles, vector_size=50, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "word2vec_model.save(\"word2vec_job_titles.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsMgWnk-RQmn"
   },
   "source": [
    "Entrena un modelo Word2Vec con los títulos de trabajo que ya tokenizaste y preprocesaste con spaCy.\n",
    "\n",
    "sentences=tokenized_titles: le das las frases como listas de palabras (tokens).\n",
    "\n",
    "vector_size=50: el modelo va a crear vectores de 50 dimensiones para cada palabra.\n",
    "\n",
    "window=5: considera un contexto de hasta 5 palabras a cada lado de la palabra central.\n",
    "\n",
    "min_count=2: solo entrena palabras que aparezcan al menos 2 veces.\n",
    "\n",
    "workers=4: usa 4 núcleos de CPU (paraleliza el trabajo).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIiHobdTRfA7"
   },
   "source": [
    "Esto guarda el modelo entrenado en un archivo con el nombre \"word2vec_job_titles.model\".\n",
    "\n",
    ".model es simplemente una convención de nombre: el archivo podría llamarse \"pepito.w2v\" y seguiría funcionando igual.\n",
    "\n",
    "Internamente, se guarda como un archivo binario que contiene:\n",
    "-Los vectores entrenados\n",
    "-La configuración del modelo\n",
    "-El vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgWlkZRFmgBF",
    "outputId": "d0541b0a-8ced-46b3-e1c2-62e3fe659931"
   },
   "outputs": [],
   "source": [
    "#Probamos el modelo, en este caso viendo las palabras más cercanas a developer\n",
    "# Cargar el modelo entrenado\n",
    "word2vec_model = Word2Vec.load(\"word2vec_job_titles.model\")\n",
    "\n",
    "# Palabras más similares a \"professional\"\n",
    "print(word2vec_model.wv.most_similar(\"professional\", topn=5))\n",
    "\n",
    "# Palabras más similares a \"human\"\n",
    "print(word2vec_model.wv.most_similar(\"director\", topn=5))\n",
    "\n",
    "print(word2vec_model.wv[\"professional\"])  # Muestra el vector de 10 dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOo8zf6jS5Lx"
   },
   "source": [
    ".wv es el acceso al vocabulario y a los vectores del modelo Word2Vec.\n",
    "Viene de word vectors, y se usa para consultar el modelo entrenado sin modificarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97PdfJruTX-4"
   },
   "source": [
    "En el primer y segundo print hay valores entre -1 y 1, que miden la similitud del coseno entre los vectores de dos palabras.\n",
    "Similitud del coseno = ¿Qué tan alineados están dos vectores en el espacio?\n",
    "\n",
    "Si el coseno es:\n",
    "1.0 -- los vectores son exactamente iguales (máxima similitud)\n",
    "0.0 -- no están relacionados (son ortogonales)\n",
    "-1.0 -- completamente opuestos (muy raro en lenguaje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_d_Q4n46J1m"
   },
   "source": [
    "## **Compare titles with query (\"human resources manager\")**\n",
    "## Ranqueo de títulos por similitud con la consulta (ej. \"Human Resources\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9RQtU9w7Ja4",
    "outputId": "f4e7dc4e-4712-42db-e37e-76bf00c5e1ff"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Función para obtener el vector promedio de un conjunto de palabras\n",
    "def get_vector(tokens, word2vec_model):\n",
    "    vectors = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "# Generar vectores para cada título de trabajo\n",
    "job_vectors = np.array([get_vector(tokens, word2vec_model) for tokens in tokenized_titles])\n",
    "\n",
    "# Tokenizar la consulta\n",
    "query_tokens = preprocess_text(\"human resources manager\")\n",
    "\n",
    "# Obtener el vector de la consulta\n",
    "query_vector = get_vector(query_tokens, word2vec_model)\n",
    "\n",
    "# Calcular similitud con cada título\n",
    "similarities = cosine_similarity([query_vector], job_vectors)[0]\n",
    "\n",
    "# Agregar la similitud al DataFrame original\n",
    "db[\"fit_score\"] = similarities\n",
    "\n",
    "# Ordenar candidatos según el puntaje de similitud\n",
    "# --- SE GENERÓ UNA REINDEXACION (el 0 es el de mayor fit_score), los ID de aquí no coinciden con la BD original ----\n",
    "db_sorted = db.sort_values(by=\"fit_score\", ascending=False).reset_index(drop=True)\n",
    "# Mostrar los mejores candidatos\n",
    "print(db_sorted[[\"job_title\", \"fit_score\"]].head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLcOaj7_Jbbs"
   },
   "source": [
    "## **Evaluation Metrics on the original ranking (similitary with query) - Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjSAAcOuJmbm",
    "outputId": "8af5c213-03d4-4bdc-d172-948522246dfe"
   },
   "outputs": [],
   "source": [
    "#GENERAMOS LAS ETIQUETAS 0 y 1, que indican si el candidatos es relevante o no\n",
    "# Agregar la columna relevance con valores nulos\n",
    "db_sorted[\"relevance\"] = None\n",
    "\n",
    "# Etiquetar manualmente los primeros 10\n",
    "manual_labels = [0, 0, 1, 1, 0, 0, 0, 0, 1, 1]  # filas 0 a 9\n",
    "db_sorted.loc[:9, \"relevance\"] = manual_labels\n",
    "\n",
    "# Mostrar los primeros 10 resultados con etiquetas\n",
    "top_10 = db_sorted.head(10)[[\"job_title\", \"fit_score\", \"relevance\"]]\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjfsAD9qgPvM"
   },
   "source": [
    "db_sorted: es el DataFrame ordenado por el fit_score.\n",
    "\n",
    "db_sorted.loc[:9, \"relevance\"]:\n",
    "selecciona las filas de la 0 a la 9 (inclusive) y la columna \"relevance\".\n",
    "Es decir: estás apuntando a los primeros 10 registros del ranking.\n",
    "\n",
    "= manual_labels:\n",
    "asignamos la lista [0, 0, 1, 1, 0, 0, 0, 0, 1, 1] a esas 10 filas en la columna \"relevance\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xkcy99VjaHm"
   },
   "outputs": [],
   "source": [
    "#DEFINIR LAS FUNCIONES DE METRICAS\n",
    "import numpy as np\n",
    "\n",
    "def precision_at_k(relevance, k):\n",
    "    \"\"\"\n",
    "    Calcula Precision@K.\n",
    "    `relevance` es una lista o array con etiquetas binarias (0 o 1).\n",
    "    \"\"\"\n",
    "    relevance_at_k = relevance[:k]\n",
    "    return np.sum(relevance_at_k) / k #suma los 1 en los primeros k elementos de relevance y los divide por k (en este caso 10).\n",
    "    #Representa la proporción de resultados relevantes en el top-k.\n",
    "\n",
    "def dcg_at_k(relevance, k):\n",
    "    \"\"\"\n",
    "    Calcula DCG@K (Discounted Cumulative Gain).\n",
    "    \"\"\"\n",
    "    relevance = np.asarray(relevance)[:k]\n",
    "    return np.sum((2**relevance - 1) / np.log2(np.arange(2, k + 2)))\n",
    "\n",
    "def ndcg_at_k(relevance, k):\n",
    "    \"\"\"\n",
    "    Calcula nDCG@K normalizando contra el DCG ideal.\n",
    "    \"\"\"\n",
    "    dcg = dcg_at_k(relevance, k)\n",
    "    ideal_relevance = sorted(relevance, reverse=True)\n",
    "    idcg = dcg_at_k(ideal_relevance, k)\n",
    "    return dcg / idcg if idcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8l6wm2-laDo"
   },
   "source": [
    "- **Discounted Cumulative Gain (DCG)** mide la utilidad de los documentos\n",
    "relevantes en un ranking, penalizando aquellos que aparecen más abajo. Cuanto más alto esté un resultado relevante, mayor su contribución al puntaje. El logaritmo penaliza la posición: a mayor profundidad en el ranking, menos valor aporta un resultado relevante.\n",
    "La función np.asarray(relevance) convierte relevance en un array de NumPy (si aún no lo es). Esto permite aplicar operaciones vectorizadas como la exponenciación y división en el cálculo del DCG.\n",
    "\n",
    "- **Normalized Discounted Cumulative Gain at k (posición k)nDCG@k** evalúa qué tan bueno es un ranking comparado con el mejor ranking posible (ideal). Es decir, normaliza el valor de DCG dividiéndolo por el IDCG (Ideal DCG), que es el DCG obtenido si los ítems relevantes estuvieran perfectamente ordenados arriba. Si el ranking coloca bien los más relevantes arriba, nDCG@k estará cerca de 1.0. Si el ranking es aleatorio o muy malo, nDCG@k estará cerca de 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebBYxTGcpA6H"
   },
   "outputs": [],
   "source": [
    "#EXTRAER LA COLUMNA DE ETIQUETAS MANUALES\n",
    "\n",
    "# Convertir la columna relevance a entero (por si quedaron como None o strings)\n",
    "relevance_labels = db_sorted[\"relevance\"].dropna().astype(int).values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daYI5Aozptdz",
    "outputId": "5d04634d-1a39-499c-e6ea-a14c8b88ddc0"
   },
   "outputs": [],
   "source": [
    "#CALCULAR LAS METRICAS\n",
    "K = 10\n",
    "prec_k = precision_at_k(relevance_labels, K)\n",
    "ndcg_k = ndcg_at_k(relevance_labels, K)\n",
    "\n",
    "print(f\"Precision@{K}: {prec_k:.3f}\")\n",
    "print(f\"nDCG@{K}: {ndcg_k:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oE5Wa4bs5fJ"
   },
   "source": [
    "## **Las métricas que se obtuvieron indican un rendimiento moderado a bajo del método de embedding Word2Vec para la query \"human resources manager\":**\n",
    "\n",
    "**Precision@10**: 0.400 Significa que 4 de los 10 primeros resultados fueron relevantes. Podría considerarse aceptable, pero no ideal si se espera alta precisión para una tarea de recomendación o búsqueda.\n",
    "\n",
    "**nDCG@10**: 0.594 La ganancia acumulada descontada está lejos de 1.0, lo que indica que los documentos más relevantes no están en las posiciones más altas del ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xt2W_EqbBXkc"
   },
   "source": [
    "##**Re-ranking of candidates choosing and ID (starred_candidates)**\n",
    "##Re-ranquear los candidatos a partir de la elección de candidatos estrella\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9CyOHqlklb7",
    "outputId": "1ce8985e-d523-4c91-f3c1-3ed67a4b0993"
   },
   "outputs": [],
   "source": [
    "# El usuario marcó como relevante el job_title con ID 68 y otros\n",
    "starred_candidates = [65, 76, 88, 80, 67]\n",
    "\n",
    "# Obtener embeddings de los candidatos estrella/ genera un array con los ID marcados\n",
    "starred_embeddings = np.array([job_vectors[id-1] for id in starred_candidates])\n",
    "\n",
    "# Calcular similitud con los títulos estrella\n",
    "# Calcula la similitud del coseno entre todos los job titles (job_vectors) y los candidatos estrella (starred_embeddings) y luego saca el promedio\n",
    "starred_similarities = cosine_similarity(job_vectors, starred_embeddings).mean(axis=1)\n",
    "\n",
    "# Ajuste de pesos: darle más peso a candidatos similares a los marcados\n",
    "final_scores = 0.7 * similarities + 0.3 * starred_similarities\n",
    "\n",
    "# Ordenar nuevamente\n",
    "re_ranked_candidates = sorted(zip(db['id'], job_titles, final_scores), key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Convertir la lista de candidatos re-rankeados en un DataFrame\n",
    "df_re_ranked = pd.DataFrame(re_ranked_candidates, columns=[\"id\", \"job_title\", \"final_score\"])\n",
    "\n",
    "# Mostrar los resultados ordenados en filas\n",
    "print(df_re_ranked.head(30))  # Muestra los 30 primeros candidatos re-rankeados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cn3IFmNB-8gC"
   },
   "source": [
    "## **Irrelevant candidates filter**\n",
    "## Filtrado de candidatos irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oH7Ls4fQ_C8l",
    "outputId": "ff13f10e-18d0-4683-cfa9-5e9557223754"
   },
   "outputs": [],
   "source": [
    "# Definir umbral de corte (percentil 10)\n",
    "cutoff = np.percentile(final_scores, 10)\n",
    "\n",
    "# Filtrar candidatos con puntajes por encima del umbral\n",
    "filtered_candidates = [c for c in re_ranked_candidates if c[2] >= cutoff]\n",
    "#c: En cada paso del for, c es una tupla como (id, job_title, final_score) de re_ranked_candidates (lista de tuplas con los candidatos)\n",
    "\n",
    "print(filtered_candidates)\n",
    "\n",
    "df_re_ranked_filtered = pd.DataFrame(filtered_candidates, columns=[\"id\", \"job_title\", \"final_score\"])\n",
    "\n",
    "# Mostrar los resultados ordenados en filas\n",
    "print(df_re_ranked_filtered.head(20))  # Muestra los 30 primeros candidatos re-rankeados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHtPcCBkGqg8"
   },
   "source": [
    "# **GLOVE IMPLEMENTATION**\n",
    "# IMPLEMENTACION DE GLOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egu6UeQON4x7"
   },
   "source": [
    "## Para adaptar el flujo de trabajo de Word2Vec a GloVe, seguiremos estos pasos:\n",
    "\n",
    "**Entrenamiento: Con GloVe, no es necesario entrenar un modelo, ya que se tienen vectores preentrenados para las palabras.**\n",
    "\n",
    "- Descargar embeddings preentrenados de GloVe (más eficiente que entrenarlo con mi dataset que es chico).\n",
    "- Cargar los embeddings y mapearlos a las palabras de los títulos de trabajo.\n",
    "- Obtener representaciones vectoriales de los títulos de trabajo.\n",
    "- Comparar los títulos con una consulta utilizando similitud del coseno.\n",
    "- Re-ranquear candidatos en función de los candidatos estrella.\n",
    "- Filtrado de candidatos irrelevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXkAfm3lY8V9"
   },
   "source": [
    "## **Glove embbedings load**\n",
    "## Carga de embbedings de GLOVE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cs6PIFJULRqc"
   },
   "source": [
    "A continuación, se descarga e instala el modelo de spaCy \"en_core_web_md\", que es un modelo de procesamiento de lenguaje natural en inglés que incluye vectores de palabras preentrenados con GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxOzHK3jJNox",
    "outputId": "2e6b387c-bb62-4bd9-d3fd-03ae1d9c7a4d"
   },
   "outputs": [],
   "source": [
    "# Cargar modelo de spaCy con vectores preentrenados de GloVe\n",
    "!python -m spacy download en_core_web_md\n",
    "#cargamos el modelo en memoria\n",
    "spacy.load(\"en_core_web_md\")\n",
    "\n",
    "import spacy\n",
    "#nlp_glove almacena el modelo de spaCy con GloVe, sin interferir con otras variables que puedas haber definido antes (como en Word2Vec)\n",
    "nlp_glove = spacy.load(\"en_core_web_md\")\n",
    "print(\"Modelo cargado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1Fj5HGNSf16"
   },
   "source": [
    "## **Text processing with GLOVE** (text or sentence)\n",
    "## Procesar texto con GLOVE (prueba con un texto o frase cualquiera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kD2elYtZLsEr",
    "outputId": "3aac5045-75be-43fa-a0ed-208be393cddf"
   },
   "outputs": [],
   "source": [
    "text = \"Machine learning is amazing.\"\n",
    "doc_gl = nlp_glove(text)\n",
    "\n",
    "for token in doc_gl:\n",
    "    print(f\"Palabra: {token.text}, Vector: {token.vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nrtsQBBfJ19"
   },
   "source": [
    "## **Pre-processing and tokenization**\n",
    "## Preprocesamiento y tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tE2-ZLgZmYj",
    "outputId": "039fb16b-5e88-4e6c-9aa2-8be3f568abf3"
   },
   "outputs": [],
   "source": [
    "# Función para preprocesar los títulos de trabajo\n",
    "def preprocess_text_glove(title):\n",
    "    doc_glove = nlp_glove(title.lower())  # Convertir a minúsculas y procesar con spaCy (modelo GloVe)\n",
    "    tokens = [token.lemma_ for token in doc_glove if token.is_alpha and not token.is_stop]  # Filtrar palabras clave\n",
    "    return tokens\n",
    "\n",
    "# Aplicar preprocesamiento con GloVe\n",
    "tokenized_titles_glove = [preprocess_text_glove(title) for title in job_titles]\n",
    "\n",
    "# Ver los títulos tokenizados\n",
    "print(tokenized_titles_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9L4xRNlbhhy"
   },
   "source": [
    "1. Función preprocess_text_glove(title):\n",
    "title es simplemente una referencia a cada elemento de la lista job_titles mientras se recorre. ¿De dónde sale el título? Cada title proviene de la lista job_titles, que a su vez proviene de la base de datos db['job_title'].\n",
    "\n",
    "2. Aplicación a la lista job_titles:\n",
    "Luego, en el paso siguiente, preprocess_text_glove se aplica a todos los títulos de trabajo que están en la lista job_titles. Es decir, la función es iterada sobre cada título de la lista y su resultado (la lista de tokens lematizados) es almacenado en una nueva lista llamada tokenized_titles_glove.\n",
    "\n",
    "- **title** es el nombre de la variable que usamos para representar un único título de trabajo dentro de la lista job_titles.\n",
    "- **job_titles** es la lista completa de títulos de trabajo, y cada vez que la iteramos con el código for title in job_titles, tomamos un solo título de la lista en cada iteración.\n",
    "- El código **preprocess_text_glove(title)** es llamado para cada title (un título de trabajo individual), y la lista **tokenized_titles_glove** almacena los resultados de la tokenización de todos los títulos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iz1wYSeofg8v"
   },
   "source": [
    "## **Model word vector**\n",
    "## Modelo de Vectores de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQi-LkJfkZRE",
    "outputId": "e3e0393f-f176-4610-a300-f8c65f870773"
   },
   "outputs": [],
   "source": [
    "# Función para obtener el vector de una palabra usando el modelo GloVe\n",
    "def get_glove_vector(word, model_glove):\n",
    "    if model_glove.vocab.has_vector(word):  # Verifica si la palabra tiene un vector\n",
    "        return model_glove(word).vector  # Devuelve el vector\n",
    "    else:\n",
    "        return np.zeros(model_glove.vector_length)  # Devuelve un vector de ceros si no está en el vocabulario\n",
    "\n",
    "# Ver el vector de alguna palabra\n",
    "print(get_glove_vector(\"resources\", nlp_glove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWaXUJ56rH7U"
   },
   "source": [
    "## **Compare titles with query (\"human resources manager\")**\n",
    "## Ranqueo de títulos por similitud con la consulta (\"human resources manager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fnxh863lqWhU",
    "outputId": "18fa34a2-a004-4a23-80eb-24844aa4cf75"
   },
   "outputs": [],
   "source": [
    "# Función para obtener el vector promedio de un conjunto de palabras usando GloVe\n",
    "def get_vector_glove(tokens, model_glove):\n",
    "    vectors_glove = [model_glove(token).vector for token in tokens if model_glove(token).vector.any()]\n",
    "    return np.mean(vectors_glove, axis=0) if vectors_glove else np.zeros(model_glove.vector_length)\n",
    "\n",
    "# Generar vectores para cada título de trabajo\n",
    "job_vectors_glove = np.array([get_vector_glove(tokens, nlp_glove) for tokens in tokenized_titles_glove])\n",
    "\n",
    "# Tokenizar la consulta\n",
    "query_tokens_glove = preprocess_text_glove(\"human resources manager\")\n",
    "\n",
    "# Obtener el vector de la consulta\n",
    "query_vector_glove = get_vector_glove(query_tokens_glove, nlp_glove)\n",
    "\n",
    "# Calcular similitud con cada título\n",
    "similarities_glove = cosine_similarity([query_vector_glove], job_vectors_glove)[0]\n",
    "\n",
    "# Agregar la similitud al DataFrame original\n",
    "db[\"fit_score\"] = similarities_glove\n",
    "\n",
    "# Ordenar candidatos según el puntaje de similitud\n",
    "# --- SE GENERÓ UNA REINDEXACION (el 0 es el de mayor fit_score), los ID de aquí no coinciden con la BD original ----\n",
    "db_sorted_glove = db.sort_values(by=\"fit_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Mostrar los mejores candidatos\n",
    "print(db_sorted_glove[[\"job_title\", \"fit_score\"]].head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdoCFLw4wZpj"
   },
   "source": [
    "## **Evaluation Metrics on the original ranking (similitary with query) - GLOVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SiiiHrKmwqYb",
    "outputId": "d6dd8391-51bd-47a3-eb32-f45370d6a04f"
   },
   "outputs": [],
   "source": [
    "# GENERAMOS LAS ETIQUETAS 0 y 1, que indican si el candidato es relevante o no\n",
    "# Agregar la columna relevance con valores nulos en el DataFrame específico de Glove\n",
    "db_sorted_glove[\"relevance\"] = None\n",
    "\n",
    "# Etiquetar manualmente los primeros 10\n",
    "manual_labels_glove = [0, 1, 1, 1, 0, 1, 0, 0, 1, 0]  # filas 0 a 9\n",
    "db_sorted_glove.loc[:9, \"relevance\"] = manual_labels_glove\n",
    "\n",
    "# Mostrar los primeros 10 resultados con etiquetas\n",
    "top_10_glove = db_sorted_glove.head(10)[[\"job_title\", \"fit_score\", \"relevance\"]]\n",
    "print(top_10_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBKL8chg57q5",
    "outputId": "533e70e0-c5a0-420e-8fa3-28583d03a11b"
   },
   "outputs": [],
   "source": [
    "# OBTENER LOS LABELS DESDE db_sorted_glove\n",
    "relevance_labels_glove = db_sorted_glove[\"relevance\"].head(10).astype(int).tolist()\n",
    "\n",
    "# DEFINIR K\n",
    "K = 10\n",
    "\n",
    "# CALCULAR LAS MÉTRICAS PARA GLOVE\n",
    "prec_k_glove = precision_at_k(relevance_labels_glove, K)\n",
    "ndcg_k_glove = ndcg_at_k(relevance_labels_glove, K)\n",
    "\n",
    "# MOSTRAR RESULTADOS\n",
    "print(f\"Precision@{K} (Glove): {prec_k_glove:.3f}\")\n",
    "print(f\"nDCG@{K} (Glove): {ndcg_k_glove:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aVHZAMT7dei"
   },
   "source": [
    "**Notas importantes:**\n",
    "\n",
    "- No hace falta redefinir las funciones precision_at_k, dcg_at_k, ni ndcg_at_k porque ya fueron definidas en la sección de WORD2VEC.\n",
    "\n",
    "- Asegurate de que la columna \"relevance\" de db_sorted_glove contenga los 10 valores manuales ya asignados.\n",
    "\n",
    "- Este código no pisa nada de Word2Vec porque usa nuevas variables (db_sorted_glove, relevance_labels_glove, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06VIZ_sL8A8h"
   },
   "source": [
    "##**CONCLUSION - Evaluación de métricas.**\n",
    "\n",
    "nDCG@10 de 0.753 es una métrica bastante alta para Glove, y efectivamente, es más alta que la obtenida para Word2Vec (que fue 0.594).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_uF_UnLr5OL"
   },
   "source": [
    "##**Re-ranking of candidates choosing and ID (starred_candidates)**\n",
    "##Re-ranquear los candidatos a partir de la elección de candidatos estrella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7i5rxQMGsT2R",
    "outputId": "cc18a0da-f146-415a-bb65-8c0308f3996d"
   },
   "outputs": [],
   "source": [
    "# El usuario marcó como relevante el job_title con ID 68 y otros\n",
    "starred_candidates_glove = [68, 69, 80, 87, 88]\n",
    "\n",
    "# Obtener embeddings de los candidatos estrella con GloVe\n",
    "starred_embeddings_glove = np.array([job_vectors_glove[id-1] for id in starred_candidates_glove])\n",
    "\n",
    "# Calcular similitud con los títulos estrella\n",
    "starred_similarities_glove = cosine_similarity(job_vectors_glove, starred_embeddings_glove).mean(axis=1)\n",
    "\n",
    "# Ajuste de pesos: darle más peso a candidatos similares a los marcados\n",
    "final_scores_glove = 0.7 * similarities_glove + 0.3 * starred_similarities_glove\n",
    "\n",
    "# Ordenar nuevamente\n",
    "re_ranked_candidates_glove = sorted(zip(db['id'], job_titles, final_scores_glove), key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Convertir la lista de candidatos re-rankeados en un DataFrame\n",
    "df_re_ranked_glove = pd.DataFrame(re_ranked_candidates_glove, columns=[\"id\", \"job_title\", \"final_score\"])\n",
    "\n",
    "# Mostrar los resultados ordenados en filas\n",
    "print(df_re_ranked_glove.head(30))  # Muestra los 30 primeros candidatos re-rankeados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5ndir8PtsNn"
   },
   "source": [
    "## **Irrelevant candidates filter**\n",
    "## Filtrado de candidatos irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwNpNN66uM4T",
    "outputId": "8cf7af4d-5c34-44f0-b67b-0ad1dd31124f"
   },
   "outputs": [],
   "source": [
    "# Definir umbral de corte (percentil 10)\n",
    "cutoff_glove = np.percentile(final_scores_glove, 10)\n",
    "\n",
    "# Filtrar candidatos con puntajes por encima del umbral\n",
    "filtered_candidates_glove = [c for c in re_ranked_candidates_glove if c[2] >= cutoff_glove]\n",
    "\n",
    "# Convertir la lista de candidatos filtrados en un DataFrame\n",
    "df_re_ranked_filtered_glove = pd.DataFrame(filtered_candidates_glove, columns=[\"id\", \"job_title\", \"final_score\"])\n",
    "\n",
    "# Mostrar los resultados ordenados en filas\n",
    "print(df_re_ranked_filtered_glove.head(20))  # Muestra los 20 primeros candidatos filtrados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YrUASE7ZA8b"
   },
   "source": [
    "# **FAST TEXT IMPLEMENTATION**\n",
    "# IMPLEMENTACION DE FAST TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9x7pSMWBTnE"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Entrenar el modelo FastText\n",
    "fasttext_model = FastText(sentences=tokenized_titles, vector_size=50, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "fasttext_model.save(\"fasttext_job_titles.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdN9qz0EC2Bs"
   },
   "source": [
    "- sentences=tokenized_titles: es la lista de tokens para cada Job Title de la base. Se hizo antes de implementar Word2Vec.\n",
    "\n",
    "- vector_size=50: igual que en Word2Vec.\n",
    "\n",
    "- La ventaja principal de FastText sobre Word2Vec es que puede generar vectores para palabras no vistas, gracias a su representación basada en sub-palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sm_szjmgFvKU",
    "outputId": "a0fccd12-8973-433a-b0c1-608f681c61fa"
   },
   "outputs": [],
   "source": [
    "# INSPECCIONAR EL MODELO\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "fasttext_model = FastText.load(\"fasttext_job_titles.model\")\n",
    "\n",
    "# Palabras más similares a \"professional\"\n",
    "print(fasttext_model.wv.most_similar(\"professional\", topn=5))\n",
    "\n",
    "# Palabras más similares a \"director\"\n",
    "print(fasttext_model.wv.most_similar(\"director\", topn=5))\n",
    "\n",
    "# Vector del término \"professional\"\n",
    "print(fasttext_model.wv[\"professional\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOi0dOLJGPwS"
   },
   "source": [
    "- Cargar el modelo entrenado.\n",
    "\n",
    "- Explorar similitud entre palabras (primero professional y luego director).\n",
    "\n",
    "- Obtener el vector de una palabra específica (professional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnGQ47B7ZnOz"
   },
   "source": [
    "## **Compare titles with query (\"Human Resources\")**\n",
    "## Ranqueo de títulos por similitud con la consulta (ej. \"Human Resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AGWY_qVGihL",
    "outputId": "451c54db-0c77-40ca-d20f-e249a4819a67"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Función para obtener el vector promedio de un conjunto de palabras\n",
    "def get_vector_fasttext(tokens, fasttext_model):\n",
    "    vectors_fasttext = [fasttext_model.wv[word] for word in tokens if word in fasttext_model.wv]\n",
    "    return np.mean(vectors_fasttext, axis=0) if vectors_fasttext else np.zeros(fasttext_model.vector_size)\n",
    "\n",
    "# Generar vectores para cada título de trabajo\n",
    "job_vectors_fasttext = np.array([get_vector_fasttext(tokens, fasttext_model) for tokens in tokenized_titles])\n",
    "\n",
    "# Tokenizar y vectorizar la consulta\n",
    "query_tokens_fasttext = preprocess_text(\"Human Resources\")\n",
    "query_vector_fasttext = get_vector_fasttext(query_tokens_fasttext, fasttext_model)\n",
    "\n",
    "# Calcular similitud con cada título\n",
    "similarities_fasttext = cosine_similarity([query_vector_fasttext], job_vectors_fasttext)[0]\n",
    "\n",
    "print(len(similarities_fasttext), len(db))\n",
    "\n",
    "# Crear copia del DataFrame y agregar la similitud\n",
    "db_fasttext = db.copy()\n",
    "db_fasttext[\"fit_score_fasttext\"] = similarities_fasttext\n",
    "\n",
    "# Ordenar candidatos según el puntaje de similitud\n",
    "db_sorted_fasttext = db_fasttext.sort_values(by=\"fit_score_fasttext\", ascending=False)\n",
    "\n",
    "# Mostrar los mejores candidatos\n",
    "print(db_sorted_fasttext[[\"job_title\", \"fit_score_fasttext\"]].head(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIzuzjvsNZO-"
   },
   "source": [
    "## **Simple Quantitative Analysis - Comparison between Word2Vec and FastText**\n",
    "## Análisis cuantitativo simple - comparación Word2Vec vs. Fast Text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPHTRO9wOJb8"
   },
   "source": [
    "## **Show Top 10 Results: Word2Vec vs. FastText**\n",
    "## Mostrar los Top 10 resultados de Word2Vec vs. FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQO-JvsdNsQg",
    "outputId": "c93c012a-894b-44f3-a8f8-ce810874cb76"
   },
   "outputs": [],
   "source": [
    "# Mostrar los 10 títulos con mayor similitud usando Word2Vec\n",
    "print(\"Top 10 resultados con Word2Vec:\")\n",
    "print(db_sorted[[\"job_title\", \"fit_score\"]].head(10))\n",
    "\n",
    "# Mostrar los 10 títulos con mayor similitud usando FastText\n",
    "print(\"\\nTop 10 resultados con FastText:\")\n",
    "print(db_sorted_fasttext[[\"job_title\", \"fit_score_fasttext\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nGBsjr5NiUtt",
    "outputId": "841894b3-9243-46c1-ccba-b60c781edfca"
   },
   "outputs": [],
   "source": [
    "print([col for col in db_fasttext.columns])\n",
    "print(len(db_fasttext[\"fit_score\"]))\n",
    "print(len(db_fasttext[\"fit_score_fasttext\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-o2VCG7MPlGL"
   },
   "source": [
    "## **Calculate the Correlation Between Similarity Scores**\n",
    "## Calcular la correlación entre los puntajes de similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6VxsmI6P1Id",
    "outputId": "13c7b367-7ede-45b1-94a1-c8807934d495"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Calcular la correlación de Pearson entre los scores de ambos modelos\n",
    "correlacion, _ = pearsonr(db_fasttext[\"fit_score\"], db_fasttext[\"fit_score_fasttext\"])\n",
    "print(f\"Correlación entre Word2Vec y FastText: {correlacion:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pi_-GHaTZD-9"
   },
   "source": [
    "- ¿Qué significa una correlación de 0.884?\n",
    "La correlación de Pearson mide la relación lineal entre dos conjuntos de valores. Va de -1 (correlación negativa perfecta) a +1 (positiva perfecta).\n",
    "\n",
    "- En mi caso:\n",
    "fit_score: similitud de Word2Vec entre la consulta y los títulos.\n",
    "fit_score_fasttext: similitud de FastText entre la misma consulta y los mismos títulos.\n",
    "\n",
    "- Un valor de 0.884 significa que:\n",
    "Ambos modelos están dando resultados muy parecidos en términos de similitud. Cuando uno da un puntaje alto, el otro también tiende a hacerlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ScNG8SwZ1wD"
   },
   "source": [
    "## **Re-ranking of candidates choosing and ID (starred_candidates)**\n",
    "## Re-ranquear los candidatos a partir de la elección de candidatos estrella*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hm_-wbYedGgk",
    "outputId": "41846b8f-a92f-4350-eb81-6f2bf0d3688b"
   },
   "outputs": [],
   "source": [
    "# Step 1: Select indexes of starred candidates (chosen manually or from top scores)\n",
    "starred_candidate_indices_fasttext = [64, 87, 98]  #\n",
    "\n",
    "# Step 2: Get FastText vectors for those starred candidates\n",
    "starred_vectors_fasttext = job_vectors_fasttext[starred_candidate_indices_fasttext]\n",
    "\n",
    "# Step 3: Compute the average vector of the starred candidates\n",
    "starred_avg_vector_fasttext = np.mean(starred_vectors_fasttext, axis=0)\n",
    "\n",
    "# Step 4: Compute cosine similarity between all job vectors and the starred average\n",
    "re_rank_similarities_fasttext = cosine_similarity([starred_avg_vector_fasttext], job_vectors_fasttext)[0]\n",
    "\n",
    "# Step 5: Create a new DataFrame to avoid overwriting\n",
    "db_reranked_fasttext = db.copy()\n",
    "db_reranked_fasttext[\"fit_score_reranked_fasttext\"] = re_rank_similarities_fasttext\n",
    "\n",
    "# Reasignar la columna 'fit_score_fasttext' desde 'db_fasttext' a 'db_reranked_fasttext'\n",
    "db_reranked_fasttext[\"fit_score_fasttext\"] = db_fasttext[\"fit_score_fasttext\"]\n",
    "\n",
    "# Ahora que tenemos la columna 'fit_score_fasttext', podemos ordenarlo\n",
    "db_sorted_reranked_fasttext = db_reranked_fasttext.sort_values(by=\"fit_score_reranked_fasttext\", ascending=False)\n",
    "\n",
    "# Mostrar los mejores candidatos\n",
    "print(db_sorted_reranked_fasttext[[\"job_title\", \"fit_score_fasttext\", \"fit_score_reranked_fasttext\"]].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqIQNNVJrBPe"
   },
   "source": [
    "# **S-BERT IMPLEMENTATION**\n",
    "# IMPLEMENTACIÓN CON S-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "6223ea1177fb4916af9cd170516c546a",
      "bb2e4121147c47279b1b311660742bba",
      "fbac8f096bfd48398abad4783ec6fc2a",
      "3f01812a9cd9477c9e0da1e9bd6602e1",
      "954048c007f84ababd280352f345f782",
      "803cccc9a3764559a7e6b73a0ea40729",
      "d46a84eafd224af5a7ae7c7f90d05960",
      "881e13edb72e4eb2ac65a57205430043",
      "aee93984de384584a0174e86fa3de6dd",
      "7382deccec3a470ea054711ee9a23b33",
      "755280cf898c41fa8c00aa031ff386b7",
      "0346d4b928a945ada24ddb296f9edea4",
      "79048220564e4d35a43e353086d1e1de",
      "1f30ccbfcac44a6785b1b4f16a1ad109",
      "6e73663692f24b529933951a19022483",
      "4cf946a4b2a14eabb993c7ddb5f53468",
      "f91cd49acaab4e8f800b06198fee674b",
      "71b27cc08edb4ff39f89c01be23c719e",
      "f197b4ce07ee455399a5c0612dbcbdde",
      "0c6cbddc0fea490e9c2d3ae715c2394f",
      "1e822ffd08e440958d1379189a06cc73",
      "bbab642cda2044f5b81221b19dfe2d7a",
      "a0d165b12fa847fea89ac7f952f8dc02",
      "9d4fe9ff8b714aec9eba8c153b63c359",
      "3b9e9ea22c82459b90f89a0d8106aab9",
      "46234dc5b80a420190387528aaa6429e",
      "2cc18a68b9844cd1b50ccb3758d37dc3",
      "16e2d8681fad49b9b7b1a326be333f30",
      "9323addefb1742caa981621e2af3b21d",
      "a1ffd749ee9d4b18baeb5e8f11501eea",
      "1ece2bc56d254d5db6f7ce4c662d557e",
      "700aa35101e441ac81cc5bcd363301b5",
      "e8776069066c443c93fb6d9ea60c5543",
      "5af2318395bc406c83587665edb360cd",
      "c18eff7062a249b082f27bd8588124ff",
      "98b70b2dde2f49f38ecb7f245ebd9935",
      "32a59bafabce499b93b5d4f92823a907",
      "091924a38bef4fd2979adf020abba7ec",
      "1821503010f6409dbcae3efbb9247860",
      "d5274637b1b24a868dc8e7064a34cbb4",
      "73771895edcd461cb395de649d27b0ff",
      "2418282505be4c3392e329c75986f154",
      "84d839d9fb074b26b730508c069e14be",
      "e9c523eab18946b686c94099c53db122",
      "6ed9fe142f8c48c096296f2649cb4abc",
      "0681284b97ab46579788f0000e0b2e07",
      "0d778ece39fd4a7f9e8dcc0cc3a96878",
      "4a05a26dfa5c4fbca46d4c2e9ee9acf7",
      "db55f30d166f46e8a440110eac74448e",
      "74ddee09a2764b28a6b5055617922978",
      "9be576fa39aa4286a44153c77eb89cb4",
      "dedbc7671b5d44b3886e96907205d5f4",
      "12f682006dba4e1fbfddfe3ee1021114",
      "b7ed57a7d5da4065b6d348122eba5dee",
      "cd4b7236272145489a8c7e4b3b5522f7",
      "ff155087dc754f49977faeb15abff48d",
      "2618effc343b4a18b6ad8766910905b0",
      "83030ece4dda4c198d37e3c61e375c0e",
      "7c0e2f26079a465daa16b695c60ed616",
      "3d24d45f104a45ca93553ba04e7e74ea",
      "fe4155d7e5cb4f60a4a1b6434be2ba89",
      "76923e6f918c4e879a521b2a882649a7",
      "90a3374aa0f54e208b40baefc1a3e3a2",
      "c9be95a35b9b434aacbf4c38141a4072",
      "5b86b7fbf1784de0ac6059ea862c0dff",
      "8530fe81e5424d818d0e3f669992f7e5",
      "947239a0b5b64087ab128e5c93424a4f",
      "82d3b893537b40279713e34543f0882d",
      "e451309e24cf4dbca1ca04743758efaf",
      "b6b0afac1bb34d999e73c23b245e62d0",
      "33994de0e395407cbb9c948e2d50c32a",
      "9fddc23c97a242fcac60ed8b75f739e1",
      "d083184b1d5b4b0685cc135a621be79e",
      "d9d4e919c9c0448d8565a3d37675eb53",
      "ad39668969f94d6480803095a6f49445",
      "53cb45506a254dff9e9ec587486651ef",
      "ae0bd7010c104bc3bf1c495e4a4054ee",
      "8fe21f6066084a2885d230442c4605b4",
      "d77fbdacb60041d18e122baf2676c861",
      "616d21a24a1f42e29fdcac125bad0c33",
      "1ad63c5a9d034b8da590b24febdab7ce",
      "e35360f26b1e4798a579c1df2784c28c",
      "dee330fbce0e40e99dc44b33f4a2e5a4",
      "506f244e1a404da7b4dd15120162f20c",
      "19037888704b48fd8d4e85ffb5cee7ea",
      "985b204203464e8fb98af505c97bb863",
      "66a5b4c77cf04fe2b32a93e0a9172663",
      "81d2d8c68aef48a8adc4d84f506f6340",
      "e75464389a6b4392ad1babec0cd9256c",
      "2e1fd778d6f143559ad8468d3b66bd5b",
      "f272bde8baf64099a62d7cbf5cce89f2",
      "207560ff04ea4466b1e10c5a7e0bd3df",
      "f914fdf3bcf643a7b45df961c45bb04c",
      "83afcf2b112440fabb8691a63b0ccbd2",
      "0418e5ef00fe4bbcb32b7417abce3ff5",
      "acdbdc8a1e774110b15f28ee0b01faca",
      "0822f8791f864cfaa4debd8ba7800959",
      "7a886f1f81c6409caf86a0b7b63ba777",
      "4c0e205ccafa492db80bd808fe187541",
      "c169d85d65114562a0a9ce4083d5111b",
      "e47b96d11c3248cc998150928b26e5ee",
      "6d61370227b747a2a0bccc4193dca19e",
      "cfd23e8b71f248ec8eaff01070e49c71",
      "8168f60fb05a4868ba1c0539e6842bcd",
      "3fbec32f06434aaea139f0428d7d0526",
      "a5f8aa99679742ea8aad683265b01071",
      "707a398092da4378abb068eedbc33086",
      "57bb668559a747058099fcea8fc3bf4b",
      "2ad2b81b1fe446c8b4ab66939fc471ff",
      "2bee076f66214fd6b4d44a3dd1cd1e39",
      "9eb43f6635dd49a79cacb1f08a2c884a",
      "f770f7b9851141ba9958f7d7f3a59cde",
      "cf15ce41a3d7411a8926d8516aeffb16",
      "9e7c46b7063c40c7bec4e15b9dd31402",
      "ff1c6af5cd3f46b6a317c77ff5e29af9",
      "141c98bdd60c4091a4e5a02578362400",
      "94b54bfed69e48a398a503a4df1096b7",
      "1c9ada47f0614a3182a8bdcb2db70577",
      "20e1e50ca00548d6b593865cbd8001fe",
      "7cc4c8c86b8b4dc589a3db78712079a1",
      "774ad1d2c8cf408e83cb78480ac3e5b7"
     ]
    },
    "id": "VhpsyaZMy2A-",
    "outputId": "32a0ef1c-ffe3-4be3-ee30-b1aab4fdf06c"
   },
   "outputs": [],
   "source": [
    "#CODE 1 - Load S-BERT model (not necesary to train from skratch)\n",
    "#CÓDIGO 1 - Cargar S-BERT (no es necesario entrenarlo desde cero)\n",
    "\n",
    "# Instalar la librería si no la tienes\n",
    "# !pip install sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pretrained S-BERT model\n",
    "# Cargar un modelo S-BERT preentrenado\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Modelo rápido y muy efectivo\n",
    "\n",
    "# (Opcional) Guardarlo localmente si quieres\n",
    "sbert_model.save('sbert_job_titles_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7b8h2__zjC0"
   },
   "source": [
    "The model all-MiniLM-L6-v2 is very common for production: is light, fast and\n",
    "precisely for embeddings of short sentences as job titles.\n",
    "\n",
    "The model all-mpnet-base-v2 is more robust (but more heavy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nM0dcHK6NbGF",
    "outputId": "3abf8f7a-27c2-4374-d59f-e3a57ebcae75"
   },
   "outputs": [],
   "source": [
    "# CODE 2 - S-BERT model test\n",
    "# CÓDIGO 2 - Prueba del modelo S-BERT\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Example job titles to test\n",
    "# Ejemplos de títulos para probar\n",
    "example_titles = [\"human resources manager\", \"human resources director\", \"software engineer\", \"talent acquisition specialist\", \"director of engineering\"]\n",
    "\n",
    "# Encode the job titles to get their embeddings\n",
    "# Obtener los embeddings de los títulos\n",
    "example_embeddings_SBert = sbert_model.encode(example_titles, normalize_embeddings=True)\n",
    "\n",
    "# Calculate cosine similarity between all pairs\n",
    "# Calcular la similitud de coseno entre todos los pares\n",
    "similarity_matrix_SBert = cosine_similarity(example_embeddings_SBert)\n",
    "\n",
    "# Show the similarity matrix\n",
    "# Mostrar la matriz de similitudes\n",
    "print(\"Similarity matrix between example titles:\")\n",
    "print(\"Matriz de similitud entre ejemplos:\")\n",
    "print(similarity_matrix_SBert)\n",
    "\n",
    "# Select the embedding of the query title directly from precomputed embeddings\n",
    "# Seleccionar el embedding de la consulta directamente de los embeddings precomputados\n",
    "query = \"human resources manager\"\n",
    "query_idx = example_titles.index(query)\n",
    "query_embedding_SBert = example_embeddings_SBert[query_idx].reshape(1, -1)\n",
    "\n",
    "# Calculate similarity between the query and all example titles\n",
    "# Calcular la similitud entre la consulta y todos los títulos de ejemplo\n",
    "similarities_SBert = cosine_similarity(query_embedding_SBert, example_embeddings_SBert)[0]\n",
    "\n",
    "# Sort titles by similarity score\n",
    "# Ordenar los títulos por puntaje de similitud\n",
    "sorted_indices = np.argsort(similarities_SBert)[::-1]  # From highest to lowest / De mayor a menor\n",
    "\n",
    "# Display most similar titles\n",
    "# Mostrar los títulos más similares\n",
    "print(\"\\nMost similar titles to 'human resources manager':\")\n",
    "print(\"\\nTítulos más similares a 'human resources manager':\")\n",
    "for idx in sorted_indices:\n",
    "    print(f\"{example_titles[idx]}: {similarities_SBert[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOwweEosOJly"
   },
   "source": [
    "**Word2Vec** trabaja a nivel de palabra. Cada palabra (\"professional\", \"director\") tiene su propio vector individual. El modelo está pensado para aprender relaciones entre palabras basadas en su contexto local (ventana de palabras cercanas).\n",
    "\n",
    "**S-BERT**, en cambio, trabaja a nivel de frases completas (\"oraciones\", \"títulos de trabajo\", \"párrafos cortos\").\n",
    "No tiene vectores individuales por palabra, sino que genera un embedding para toda la oración o frase.\n",
    "- Por eso se tomaron frases como \"human resources manager\", \"software engineer\", etc., y se generó un solo vector por frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVPX0b17XoUG"
   },
   "source": [
    "En la matriz de similitud\n",
    "\n",
    "Cada fila representa un título de trabajo (embedding).\n",
    "\n",
    "Cada columna representa también un título de trabajo (embedding).\n",
    "\n",
    "El valor en la posición (i, j) de la matriz es la similitud del coseno entre el título i y el título j.\n",
    "\n",
    "La diagonal principal (cuando i == j) es la comparación de un título consigo mismo, siempre es 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtkqHP-1YMGb"
   },
   "source": [
    "**sbert_model.encode** genera resultados aleatorios pequeños si no se setea **normalize_embeddings=True**.\n",
    "Esto significa que los vectores no están normalizados y, por tanto, su similitud consigo mismos no es exactamente 1. Luego de normalizar, la similitud de un vector consigo mismo se convirtió en 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8pPd7xb2aoTe",
    "outputId": "8b661f22-bc50-4595-d0b6-a09b89540a9a"
   },
   "outputs": [],
   "source": [
    "db.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bNL2NkfnNzC"
   },
   "source": [
    "## **Compare titles with query (\"human resources manager\")**\n",
    "## Ranqueo de títulos por similitud con la consulta (ej. \"human resources manager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMQGRwrcZtcU",
    "outputId": "5f84e842-3840-4130-a838-23ef027e8423"
   },
   "outputs": [],
   "source": [
    "# CODE 3 - Ranking job titles by similarity to the query using S-BERT\n",
    "# CÓDIGO 3 - Ranqueo de títulos de trabajo por similitud a la consulta usando S-BERT\n",
    "\n",
    "# Function to preprocess and encode job titles\n",
    "# Función para preprocesar y codificar títulos de trabajo\n",
    "def encode_titles_SBert(titles_list):\n",
    "    return sbert_model.encode(titles_list, normalize_embeddings=True)\n",
    "\n",
    "# Generate embeddings for all job titles in the database\n",
    "# Generar embeddings para todos los títulos en la base de datos\n",
    "job_titles_SBert = db['job_title'].tolist()\n",
    "job_embeddings_SBert = encode_titles_SBert(job_titles_SBert)\n",
    "\n",
    "# Preprocess and encode the query\n",
    "# Preprocesar y codificar la consulta\n",
    "query_SBert = \"human resources manager\"\n",
    "query_embedding_SBert = sbert_model.encode([query_SBert], normalize_embeddings=True)\n",
    "\n",
    "# Calculate cosine similarity between the query and all job titles\n",
    "# Calcular la similitud de coseno entre la consulta y todos los títulos\n",
    "similarities_SBert = cosine_similarity(query_embedding_SBert, job_embeddings_SBert)[0]\n",
    "\n",
    "# Add the similarity scores to the original DataFrame\n",
    "# Agregar los puntajes de similitud al DataFrame original\n",
    "db[\"fit_score_SBert\"] = similarities_SBert\n",
    "\n",
    "# Sort candidates based on the fit score\n",
    "# Ordenar candidatos basados en el puntaje de ajuste\n",
    "db_sorted_SBert = db.sort_values(by=\"fit_score_SBert\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the top candidates\n",
    "# Mostrar los mejores candidatos\n",
    "print(db_sorted_SBert[[\"job_title\", \"fit_score_SBert\"]].head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCvzptFzdBcz"
   },
   "source": [
    "- Primero definimos una función encode_titles_SBert para codificar listas de títulos usando S-BERT, con normalización.\n",
    "\n",
    "- Generamos los embeddings para todos los job titles que tenés en tu base de datos.\n",
    "\n",
    "- Codificamos el query \"Human Resources\" usando también normalize_embeddings=True.\n",
    "\n",
    "- Calculamos la similaridad del coseno entre la consulta y cada job title.\n",
    "\n",
    "- Guardamos los resultados en una nueva columna llamada \"fit_score_SBert\" en tu DataFrame.\n",
    "\n",
    "- Finalmente ordenamos de mayor a menor similitud y mostramos los 50 mejores candidatos (hay reindexación de los ID, el 0 es el de mayor fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvgfpZmLf0sO"
   },
   "source": [
    "**Algunos puntos importantes:**\n",
    "\n",
    "**Primero** (1):\n",
    "\n",
    "titles_list: nombre interno del parámetro\n",
    "\n",
    "job_titles_SBert: la lista real que le pasamos cuando llamamos a la función\n",
    "\n",
    "**Segundo** (2):\n",
    "\n",
    "¿Qué hace .tolist()?\n",
    " .tolist() es un método de pandas que convierte una columna (que es un objeto especial de tipo Series) en una lista de Python común y corriente. Lo hacemos porque S-BERT espera listas de textos, no una Series de pandas.\n",
    "\n",
    "**Tercero** (3):\n",
    "\n",
    "Para codificar muchos títulos (todos los job_titles del DataFrame), usamos la función encode_titles_SBert(), que adentro llama a sbert_model.encode() con normalize_embeddings=True.\n",
    "\n",
    "Para codificar un solo query (\"Human Resources\"), como es solo un string y no queremos crear otra función, directamente usamos:\n",
    "query_embedding_SBert = **sbert_model.encode**([query_SBert], normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdGzPFOiH2Oi"
   },
   "source": [
    "##**Evaluation Metrics on the original ranking (similitary with query) - S-BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-0hIYgKII8A",
    "outputId": "a8282f2a-920b-4f15-a6ab-d8e74fa730b9"
   },
   "outputs": [],
   "source": [
    "# GENERAMOS LAS ETIQUETAS 0 y 1, que indican si el candidato es relevante o no\n",
    "# Agregar la columna relevance con valores nulos en el DataFrame específico de S-BERT\n",
    "db_sorted_SBert[\"relevance\"] = None\n",
    "\n",
    "# Etiquetar manualmente los primeros 10\n",
    "manual_labels_SBert = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]  # filas 0 a 9\n",
    "db_sorted_SBert.loc[:9, \"relevance\"] = manual_labels_SBert\n",
    "\n",
    "# Mostrar los primeros 10 resultados con etiquetas\n",
    "top_10_SBert = db_sorted_SBert.head(10)[[\"job_title\", \"fit_score_SBert\", \"relevance\"]]\n",
    "print(top_10_SBert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gFDoExkXKwSL",
    "outputId": "748e6119-e590-4275-debe-4c3270d16bec"
   },
   "outputs": [],
   "source": [
    "# Asegurarse de que los valores en relevance no sean None y convertirlos a int\n",
    "relevance_labels_SBert = db_sorted_SBert[\"relevance\"].fillna(0).astype(int).tolist()\n",
    "\n",
    "# Definir K\n",
    "K = 10\n",
    "\n",
    "# Calcular métricas\n",
    "prec_k_SBert = precision_at_k(relevance_labels_SBert, K)\n",
    "ndcg_k_SBert = ndcg_at_k(relevance_labels_SBert, K)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Precision@{K} (SBert): {prec_k_SBert:.3f}\")\n",
    "print(f\"nDCG@{K} (SBert): {ndcg_k_SBert:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzZ06g0GNWQk"
   },
   "source": [
    "##**CONCLUSIONES - Evaluación de Métricas - S-Bert**\n",
    "\n",
    "- Un valor de **nDCG@10 = 1.000** significa que el modelo ordenó perfectamente los candidatos según tus etiquetas manuales. Puso primero los más relevantes, luego los menos, sin errores en la jerarquía.\n",
    "\n",
    "- **Precision@10 = 0.200**: Significa que de los primeros 10 candidatos, solo 2 fueron marcados como relevantes. O sea, el modelo ordenó bien a los pocos que sí eran relevantes, pero el resto del top 10 no fue útil. El modelo puso en primer lugar a los mejores pero la mayoría del top 10 no era relevante según tu criterio.\n",
    "\n",
    "- Para S-BERT, **\"Aspiring Human Resources Specialist\"** está semánticamente muy cerca de \"Human Resources Manager\" -- comparten contexto, área y muchas palabras clave.\n",
    "\n",
    "**Conclusión conceptual**\n",
    "nDCG alto, Precision baja: S-BERT es muy bueno entendiendo significado general y ordenando lo que considera relevante, pero no siempre alinea ese juicio con la necesidad específica (e.g., que un \"aspirante\" no es apto aún).\n",
    "\n",
    "Esto revela que los embeddings preentrenados no siempre reflejan los criterios personalizados de relevancia.\n",
    "\n",
    "Requiere ajuste fino o re-entrenamiento en dominios específicos, o combinarlo con filtros adicionales (e.g., excluir títulos con “aspiring”).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvm-x7IJnmzM"
   },
   "source": [
    "##**Re-ranking of candidates choosing and ID (starred_candidates)**\n",
    "##Re-ranquear los candidatos a partir de la elección de candidatos estrella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_U9SSeXf6W4",
    "outputId": "46a9e40d-8fb7-4c72-8406-0687a3048e01"
   },
   "outputs": [],
   "source": [
    "# CODE 4 - Re-ranking of candidates after selecting starred candidates\n",
    "# CÓDIGO 4 - Re-ranqueo de candidatos después de seleccionar candidatos estrella\n",
    "\n",
    "# Define the IDs of the starred candidates\n",
    "# Definir los IDs de los candidatos marcados como estrella\n",
    "starred_candidates_SBert = [73, 87, 50, 67, 83]\n",
    "\n",
    "# Get embeddings of the starred candidates\n",
    "# Obtener los embeddings de los candidatos estrella\n",
    "starred_embeddings_SBert = np.array([job_embeddings_SBert[id-1] for id in starred_candidates_SBert])\n",
    "\n",
    "# Calculate similarity between all job titles and the starred candidates\n",
    "# Calcular la similitud entre todos los títulos de trabajo y los candidatos estrella\n",
    "starred_similarities_SBert = cosine_similarity(job_embeddings_SBert, starred_embeddings_SBert).mean(axis=1)\n",
    "\n",
    "# Adjust the final scores: give more weight to candidates similar to the starred ones\n",
    "# Ajustar los puntajes finales: dar más peso a los candidatos similares a los marcados\n",
    "final_scores_SBert = 0.7 * similarities_SBert + 0.3 * starred_similarities_SBert\n",
    "\n",
    "# Re-rank candidates based on the new scores\n",
    "# Re-ranquear los candidatos basándose en los nuevos puntajes\n",
    "re_ranked_candidates_SBert = sorted(zip(db['id'], job_titles_SBert, final_scores_SBert), key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Convert the re-ranked list into a DataFrame\n",
    "# Convertir la lista re-ranqueada en un DataFrame\n",
    "df_re_ranked_SBert = pd.DataFrame(re_ranked_candidates_SBert, columns=[\"id\", \"job_title\", \"final_score\"])\n",
    "\n",
    "# Show the top re-ranked candidates\n",
    "# Mostrar los principales candidatos re-ranqueados\n",
    "print(df_re_ranked_SBert.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnR6QYQzjTVb"
   },
   "source": [
    "**OBSERVACIÓN**\n",
    "\n",
    "¿Por qué usamos zip(db['id'], job_titles_SBert, final_scores_SBert) y no solo final_scores_SBert? Porque no solo queremos ordenar los puntajes, sino también conservar la información asociada a cada puntaje: el ID del candidato, el título de trabajo (job title), y el puntaje final (final score).\n",
    "\n",
    "zip() lo que hace es empaquetar los tres elementos juntos en tuplas.\n",
    "\n",
    "Cada tupla luce así:\n",
    "\n",
    "(15, \"software engineer\", 0.8654)\n",
    "\n",
    "(68, \"human resources manager\", 0.9432)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVIgjunXn7dI"
   },
   "source": [
    "## **Irrelevant candidates filter (percentil 10)**\n",
    "## Filtrado de candidatos irrelevantes (percentil 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3q2LPMEl0Zw",
    "outputId": "52944a2d-c032-414b-ccab-13c4bb8e3143"
   },
   "outputs": [],
   "source": [
    "# CODE 5 - Candidate filtering based on cutoff\n",
    "# CÓDIGO 5 - Filtrado de candidatos basado en umbral\n",
    "\n",
    "# Define a cutoff threshold (e.g., 10th percentile)\n",
    "# Definir un umbral de corte (por ejemplo, percentil 10)\n",
    "cutoff_SBert = np.percentile(final_scores_SBert, 10)\n",
    "\n",
    "# Filter candidates with scores above the threshold\n",
    "# Filtrar candidatos con puntajes por encima del umbral\n",
    "filtered_candidates_SBert = [c for c in re_ranked_candidates_SBert if c[2] >= cutoff_SBert]\n",
    "\n",
    "# Convert the filtered list to a DataFrame\n",
    "# Convertir la lista filtrada en un DataFrame\n",
    "df_re_ranked_filtered_SBert = pd.DataFrame(filtered_candidates_SBert, columns=[\"id\", \"job_title\", \"final_score\"])\n",
    "\n",
    "# Show the top filtered candidates\n",
    "# Mostrar los principales candidatos filtrados\n",
    "print(df_re_ranked_filtered_SBert.head(20)) # Show top 20 candidates / Mostrar los 20 mejores candidatos"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
